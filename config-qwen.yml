base_model: Qwen/Qwen2-7B-Instruct
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer
hub_model_id: bkciccar/qwen2-7B-culture-fft

load_in_8bit: false
load_in_4bit: false
strict: false

datasets:
  - path: json
    data_files: culture_shuffle.jsonl
    split: train
    type: alpaca

test_datasets:
  - path: json
    data_files:
      - culturalbench-hard-preprocessed.jsonl
    split: train
    type:
      system_prompt: Below is a question with a potential answer. Please respond with only 'true' or 'false'.
      field_system:
      field_instruction: prompt_question
      field_input: prompt_option
      field_output: answer
      format: |-
        ### Question:
        {instruction}
        
        ### Option:
        {input}
        
        ### Response (true or false):

dataset_prepared_path:
output_dir: /scratch/bkciccar/outputs/qwen2-7b-fft

sequence_len: 4096
sample_packing: true
eval_sample_packing: true
pad_to_sequence_len: true

wandb_project: "Meta-Llama-3.1-8B_CultureBank_FineTuning"
wandb_entity: "bcicc"
wandb_watch: "all"
wandb_name: "Qwen_FFT_02"
wandb_log_model: "end"

gradient_accumulation_steps: 4
micro_batch_size: 2
eval_batch_size: 10
num_epochs: 4
optimizer: paged_adamw_8bit
lr_scheduler: cosine
learning_rate: 1e-5

train_on_inputs: false
group_by_length: false
bf16: auto
fp16:
tf32: false

gradient_checkpointing: true
early_stopping_patience:
resume_from_checkpoint:
local_rank:
logging_steps: 1
xformers_attention:
flash_attention: true
s2_attention:

warmup_steps: 10
eval_table_size:
eval_max_new_tokens: 5
evals_per_epoch: 16
saves_per_epoch: 1
debug:
deepspeed:
weight_decay: 0.0
fsdp:
fsdp_config:
special_tokens:
  pad_token: <|end_of_text|>

overrides_of_trainer_kwargs:
  compute_metrics: "custom_metrics.compute_metrics"
