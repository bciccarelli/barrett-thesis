base_model: NousResearch/Meta-Llama-3.1-8B
model_type: LlamaForCausalLM
tokenizer_type: AutoTokenizer
hub_model_id: bkciccar/custom_model_name

load_in_8bit: false
load_in_4bit: false
strict: false

datasets:
  - path: json
    data_files: culture_shuffle.jsonl
    split: train
    type: alpaca

  # - path: SALT-NLP/CultureBank
  #   split: reddit
  #   type:
  #     system_prompt: "You are a cultural expert."
  #     field_system:
  #     field_instruction: eval_question
  #     field_input:
  #     field_output: eval_whole_desc
  #     format: |-
  #       ### Instruction:
  #       {instruction}
        
  #       ### Input:
  #       {input}
        
  #       ### Response:
  #     no_input_format: |-
  #       ### Instruction:
  #       {instruction}
        
  #       ### Response:
test_datasets:
  - path: json
    data_files:
      - culturalbench-hard-preprocessed.jsonl
    split: train
    type:
      system_prompt: Below is a question with a potential answer. Please respond with only 'true' or 'false'.
      field_system:
      field_instruction: prompt_question
      field_input: prompt_option
      field_output: answer
      format: |-
        ### Question:
        {instruction}
        
        ### Option:
        {input}
        
        ### Response (true or false):

dataset_prepared_path:
# output_dir: ./outputs/llama-3.1-8b-lora
output_dir: ./outputs/llama-3.1-8b-fft

sequence_len: 4096
sample_packing: true
eval_sample_packing: true
pad_to_sequence_len: true

# adapter: lora
# lora_model_dir:
# lora_r: 32
# lora_alpha: 16
# lora_dropout: 0.05
# lora_target_linear: true
# lora_fan_in_fan_out: false
# lora_modules_to_save:
#   - embed_tokens
#   - lm_head

wandb_project: "Meta-Llama-3.1-8B_CultureBank_FineTuning"
wandb_entity: "bcicc"
wandb_watch: "all"
wandb_name: "Full_FineTuning_Run_01"
wandb_log_model: "checkpoint"

gradient_accumulation_steps: 4
micro_batch_size: 2
eval_batch_size: 10
num_epochs: 4
optimizer: paged_adamw_8bit
lr_scheduler: cosine
learning_rate: 2e-5

train_on_inputs: false
group_by_length: false
bf16: auto
fp16:
tf32: false

gradient_checkpointing: true
early_stopping_patience:
resume_from_checkpoint:
local_rank:
logging_steps: 1
xformers_attention:
flash_attention: true
s2_attention:

warmup_steps: 10
eval_table_size:
eval_max_new_tokens: 5
evals_per_epoch: 8
saves_per_epoch: 1
debug:
deepspeed:
weight_decay: 0.0
fsdp:
fsdp_config:
special_tokens:
  pad_token: <|end_of_text|>

overrides_of_trainer_kwargs:
  compute_metrics: "custom_metrics.compute_metrics"
